{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/131_data/jihwan/2024_lgm/vq-clip/.vq-clip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/131_data/jihwan/2024_lgm/vq-clip/.vq-clip/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do vq-clip lazy init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VQCLIPModel(\n",
       "  (clip_model): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 768)\n",
       "        (position_embedding): Embedding(77, 768)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(257, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       "  )\n",
       "  (vision_vq_adapter): VQAdapterModel(\n",
       "    (vq): VectorQuantize(\n",
       "      (project_in): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      (project_out): Linear(in_features=1024, out_features=768, bias=True)\n",
       "      (_codebook): EuclideanCodebook()\n",
       "    )\n",
       "    (in_feature_net): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1028, bias=False)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1028,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Block(\n",
       "        (norm): LayerNorm((1028,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc1): Linear(in_features=1028, out_features=512, bias=False)\n",
       "          (c_fc2): Linear(in_features=1028, out_features=512, bias=False)\n",
       "          (c_proj): Linear(in_features=512, out_features=1028, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Linear(in_features=1028, out_features=768, bias=False)\n",
       "    )\n",
       "    (out_feature_net): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vq_clip import VQCLIPModel\n",
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = VQCLIPModel.from_pretrained_clip(clip_path=\"openai/clip-vit-large-patch14\", vision_vq_adapter_path=\"adams-story/vq-ViT-L-14-k64-d32-ema\", )\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_tensor_video_clip(clip):\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tensor. Got %s\" % type(clip))\n",
    "\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])\n",
    "\n",
    "\n",
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if len(clip.size()) != 4:\n",
    "        raise ValueError(\"clip should be a 4D tensor\")\n",
    "    return clip[..., i: i + h, j: j + w]\n",
    "\n",
    "\n",
    "def resize(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    return torch.nn.functional.interpolate(clip, size=target_size, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resize_scale(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    H, W = clip.size(-2), clip.size(-1)\n",
    "    scale_ = target_size[0] / min(H, W)\n",
    "    return torch.nn.functional.interpolate(clip, scale_factor=scale_, mode=interpolation_mode, align_corners=True, antialias=True)\n",
    "\n",
    "\n",
    "def resized_crop(clip, i, j, h, w, size, interpolation_mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Do spatial cropping and resizing to the video clip\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
    "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
    "        h (int): Height of the cropped region.\n",
    "        w (int): Width of the cropped region.\n",
    "        size (tuple(int, int)): height and width of resized clip\n",
    "    Returns:\n",
    "        clip (torch.tensor): Resized and cropped clip. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    clip = crop(clip, i, j, h, w)\n",
    "    clip = resize(clip, size, interpolation_mode)\n",
    "    return clip\n",
    "\n",
    "\n",
    "def center_crop(clip, crop_size):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    th, tw = crop_size\n",
    "    if h < th or w < tw:\n",
    "        raise ValueError(\"height and width must be no smaller than crop_size\")\n",
    "\n",
    "    i = int(round((h - th) / 2.0))\n",
    "    j = int(round((w - tw) / 2.0))\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "\n",
    "def center_crop_using_short_edge(clip):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    if h < w:\n",
    "        th, tw = h, h\n",
    "        i = 0\n",
    "        j = int(round((w - tw) / 2.0))\n",
    "    else:\n",
    "        th, tw = w, w\n",
    "        i = int(round((h - th) / 2.0))\n",
    "        j = 0\n",
    "    return crop(clip, i, j, th, tw)\n",
    "\n",
    "class CenterCropResizeVideo:\n",
    "    '''\n",
    "    First use the short side for cropping length,\n",
    "    center crop video, then resize to the specified size\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size,\n",
    "            interpolation_mode=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            if len(size) != 2:\n",
    "                raise ValueError(f\"size should be tuple (height, width), instead got {size}\")\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        Returns:\n",
    "            torch.tensor: scale resized / center cropped video clip.\n",
    "                size is (T, C, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        clip_center_crop = center_crop_using_short_edge(clip)\n",
    "        clip_center_crop_resize = resize(clip_center_crop, target_size=self.size,\n",
    "                                         interpolation_mode=self.interpolation_mode)\n",
    "        return clip_center_crop_resize\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size}, interpolation_mode={self.interpolation_mode}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 3, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decord import VideoReader, cpu\n",
    "from decord import bridge\n",
    "\n",
    "bridge = bridge.set_bridge(\"torch\")\n",
    "\n",
    "video_path = \"/cvdata1/jihwan/minecraft/train/1_2/000002.mp4\"\n",
    "\n",
    "vr = VideoReader(video_path, ctx=cpu(0))\n",
    "video = vr.get_batch(range(len(vr)))\n",
    "video = video.permute(0, 3, 1, 2)\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 3, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_crop_resize = CenterCropResizeVideo(size=128)\n",
    "resized_video = center_crop_resize(video)\n",
    "resized_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1, len(resized_video)):\n",
    "        image = resized_video[i]\n",
    "        image = Image.fromarray(image.permute(1, 2, 0).numpy())\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(model.device)\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        \n",
    "        embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive frames\n",
      "tensor([0.8687], device='cuda:0')\n",
      "tensor([0.8255], device='cuda:0')\n",
      "tensor([0.8702], device='cuda:0')\n",
      "tensor([0.8546], device='cuda:0')\n",
      "tensor([0.8461], device='cuda:0')\n",
      "tensor([0.8447], device='cuda:0')\n",
      "tensor([0.8723], device='cuda:0')\n",
      "tensor([0.9206], device='cuda:0')\n",
      "tensor([0.8982], device='cuda:0')\n",
      "tensor([0.8687], device='cuda:0')\n",
      "tensor([0.8400], device='cuda:0')\n",
      "tensor([0.8530], device='cuda:0')\n",
      "tensor([0.8482], device='cuda:0')\n",
      "tensor([0.7813], device='cuda:0')\n",
      "tensor([0.7504], device='cuda:0')\n",
      "tensor([0.7977], device='cuda:0')\n",
      "tensor([0.7753], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "print(\"consecutive frames\")\n",
    "for i in range(1, 18):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consecutive chunks\n",
      "tensor([0.7753], device='cuda:0')\n",
      "tensor([0.7953], device='cuda:0')\n",
      "tensor([0.7641], device='cuda:0')\n",
      "tensor([0.7816], device='cuda:0')\n",
      "tensor([0.8147], device='cuda:0')\n",
      "tensor([0.7802], device='cuda:0')\n",
      "tensor([0.7830], device='cuda:0')\n",
      "tensor([0.7938], device='cuda:0')\n",
      "tensor([0.7787], device='cuda:0')\n",
      "tensor([0.7576], device='cuda:0')\n",
      "tensor([0.8617], device='cuda:0')\n",
      "tensor([0.9019], device='cuda:0')\n",
      "tensor([0.7302], device='cuda:0')\n",
      "tensor([0.7915], device='cuda:0')\n",
      "tensor([0.7933], device='cuda:0')\n",
      "tensor([0.8073], device='cuda:0')\n",
      "tensor([0.7488], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"consecutive chunks\")\n",
    "for i in range(17, len(embeddings), 17):\n",
    "    print(cosine_sim(embeddings[0], embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison btw the first frames of different videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8118], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center_crop_resize = CenterCropResizeVideo(size=128)\n",
    "video_path_1 = \"/cvdata1/jihwan/minecraft/train/1_2/000000.mp4\"\n",
    "video_path_2 = \"/cvdata1/jihwan/minecraft/train/1_2/000002.mp4\"\n",
    "\n",
    "vr1 = VideoReader(video_path_1, ctx=cpu(0))\n",
    "video1 = vr1.get_batch(range(len(vr1)))\n",
    "video1 = video1.permute(0, 3, 1, 2)\n",
    "resized_video1 = center_crop_resize(video1)\n",
    "\n",
    "\n",
    "vr2 = VideoReader(video_path_2, ctx=cpu(0))\n",
    "video2 = vr2.get_batch(range(len(vr2)))\n",
    "video2 = video2.permute(0, 3, 1, 2)\n",
    "resized_video2 = center_crop_resize(video2)\n",
    "\n",
    "\n",
    "image1 = resized_video1[i]\n",
    "image1 = Image.fromarray(image1.permute(1, 2, 0).numpy())\n",
    "inputs1 = processor(images=image1, return_tensors=\"pt\")\n",
    "inputs1 = inputs1.to(model.device)\n",
    "embedding1 = model.get_image_features(**inputs1)\n",
    "\n",
    "image2 = resized_video2[i]\n",
    "image2 = Image.fromarray(image2.permute(1, 2, 0).numpy())\n",
    "inputs2 = processor(images=image2, return_tensors=\"pt\")\n",
    "inputs2 = inputs2.to(model.device)\n",
    "embedding2 = model.get_image_features(**inputs2)\n",
    "\n",
    "cosine_sim(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
